{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_search_engine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbu39lGDBa5u51mFMKJgmZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nahidosen/ProblemSolving/blob/main/simple_search_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Search Engine\n",
        "\n",
        "The goal of this assignment is to create a simple search engine in any language (preferably Python). The assignment should be completed following the OOP concept. The search engine should be implemented as an inverted index (http://en.wikipedia.org/wiki/Inverted_index) that runs in memory and can return a result list that is sorted by TF-IDF (http://en.wikipedia.org/wiki/Tf*idf). \n",
        "\n",
        "\n",
        "## The search engine should:\n",
        "\n",
        "* be able to take in a list of documents\n",
        "* support searches for single terms in the document set (http://en.wikipedia.org/wiki/Tokenization)\n",
        "* return a list of matching documents sorted by TF-IDF.\n",
        "* For TF choose either (using Wikipedia terminology):\n",
        "* * term frequency adjusted for document length: ft,d ÷ (number of words in d)\n",
        "* * augmented frequency\n",
        "* For IDF choose (using Wikipedia terminology):\n",
        "\n",
        "\n",
        "## Example\n",
        "\n",
        "The following documents are indexed:\n",
        "\n",
        "* Document 1: “the brown fox jumped over the brown dog”\n",
        "* Document 2: “the lazy brown dog sat in the corner”\n",
        "* Document 3: “the red fox bit the lazy dog”\n",
        "\n",
        "* A search for “brown” should now return the list: [document 1, document 2].\n",
        "* A search for “fox” should return the list: [document 3, document 1]\n",
        "\n",
        "NOTE: The search engine does not need to persist the index to disk; a simple implementation in memory is fine.\n",
        "A document need only be a simple String. No GUI is needed, but you should be able to write a query and get a result back."
      ],
      "metadata": {
        "id": "N8Y6BTSBLyBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################  NOTES  ###################################\n",
        "\n",
        "# Requirements_1 :::\n",
        "# implement search engine as an inverted index\n",
        "# Runs in Memory\n",
        "# Returns Result List\n",
        "# Sort by TF-IDF\n",
        "\n",
        "# Requirements_2 :::\n",
        "# Take List of Documents\n",
        "# Support single term search (Tokenization)\n",
        "# Return List of Matching Documents sorted by TF-IDF\n",
        "\n",
        "# Requirements TF-IDF :::\n",
        "# TF Terminology \n",
        "# 1. wiki\n",
        "# 2. term frequency adjusted for document length: ft,d ÷ (number of words in d)\n",
        "# 3. augmented frequency\n",
        "# IDF Terminology => wiki\n",
        "\n",
        "# Example\n",
        "# The following documents are indexed:\n",
        "# Document 1: “the brown fox jumped over the brown dog”\n",
        "# Document 2: “the lazy brown dog sat in the corner”\n",
        "# Document 3: “the red fox bit the lazy dog”\n",
        "# A search for “brown” should now return the list: [document 1, document 2].\n",
        "# A search for “fox” should return the list: [document 3, document 1]\n",
        "\n",
        "# NOTE: \n",
        "# 1. The search engine does not need to persist the index to disk; \n",
        "#    a simple implementation in memory is fine.\n",
        "# 2. A document need only be a simple String. \n",
        "#    No GUI is needed, but you should be able to write a query and get a result back.\n",
        "# From example. QUERY = 1 WORD\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "\n",
        "# create database as an Inverted Index + RUNS in memory\n",
        "# Match input in database\n",
        "# TF-IDF Sorting Function\n",
        "# input documents\n",
        "# input query\n",
        "# return results"
      ],
      "metadata": {
        "id": "5iXkcDbXm3Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import math\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgff4rQKOF1T",
        "outputId": "9a2205a2-9f0a-4a1f-8681-fc8d523056ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX1vmVuZOCsO"
      },
      "outputs": [],
      "source": [
        "\n",
        "class simple_search_engine():\n",
        "    def __init__(self):\n",
        "        self.doc = []\n",
        "        self.inverse_index = {}\n",
        "        self.word_freq = {}\n",
        "        self.tf_idf = {}\n",
        "\n",
        "\n",
        "        # demo data\n",
        "        Document_1 = 'the brown fox jumped over the brown dog'\n",
        "        Document_2 = 'the lazy brown dog sat in the corner'\n",
        "        Document_3 = 'the red fox bit the lazy dog'\n",
        "        \n",
        "        self.doc_list = [[Document_1], [Document_2], [Document_3]]\n",
        "    \n",
        "    def add_doc(self):\n",
        "\n",
        "        for i in self.doc_list:\n",
        "            self.doc.append([i])\n",
        "        self.initialize()\n",
        "        \n",
        "\n",
        "    def add_to_doc(self):\n",
        "        input_please = input(\"Please enter a Document\")\n",
        "        self.doc.append([input_please])\n",
        "        self.initialize()\n",
        "        \n",
        "\n",
        "    def initialize(self):\n",
        "        self.inverse_idx()\n",
        "        self.word_fq()\n",
        "        self.tf_idf_weight()\n",
        "\n",
        "\n",
        "    def inverse_idx(self):\n",
        "\n",
        "        if len(self.doc) > 0:\n",
        "            for i, docs in enumerate(self.doc):\n",
        "                self.doc[i].append(len(word_tokenize(self.doc[i])))    #Error\n",
        "\n",
        "                for item in word_tokenize(docs[0]):\n",
        "                    if item in self.inverse_index:\n",
        "                        self.inverse_index[item].add(i)\n",
        "                    else: \n",
        "                        self.inverse_index[item] = {i}\n",
        "\n",
        "\n",
        "    def word_fq(self):\n",
        "\n",
        "        self.word_freq = self.inverse_index.copy()        \n",
        "        for key, value in self.inverse_index:\n",
        "            self.word_freq[key] = list(value)\n",
        "\n",
        "        for key, value in self.word_freq:\n",
        "            tf = []\n",
        "            for i in range(value):\n",
        "                tf[i] = [word_tokenize(self.doc[i][0]).count(key) / self.doc[i][1], value[i]]\n",
        "            self.word_freq[key] = tf\n",
        "            \n",
        "\n",
        "    def tf_idf_weight(self):\n",
        "\n",
        "        self.tf_idf = self.word_freq.copy()\n",
        "        \n",
        "        for key, value in self.word_freq:\n",
        "            tf__idf = []\n",
        "            for i in range(value):\n",
        "                tf__idf[i] = value[0] * math.log(len(self.doc)  / len(value[1]))\n",
        "            self.tf_idf[key] = tf__idf\n",
        "\n",
        "        \n",
        "    def retrieve(self):\n",
        "        \n",
        "        is_there = self.inverse_index.get(input())\n",
        "\n",
        "        if is_there is None:\n",
        "            print(\"No data\")\n",
        "        else:\n",
        "            for i in is_there:\n",
        "                print('document', i)\n",
        "\n",
        "    \n",
        "search_it = simple_search_engine()\n",
        "\n",
        "search_it.add_doc()     #to initialize demo data (must)\n",
        "search_it.add_to_doc()  #to insert document \n",
        "search_it.retrieve()    #enter a query to get document "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Document_1 = 'the brown fox jumped over the brown dog'\n",
        "Document_2 = 'the lazy brown dog sat in the corner'\n",
        "Document_3 = 'the red fox bit the lazy dog'\n",
        "doc = [[Document_1], [Document_2], [Document_3]]\n",
        "\n",
        "print(len(doc))\n",
        "\n",
        "\n",
        "if len(doc) > 0:\n",
        "  for i, docs in enumerate(doc):\n",
        "    doc[i].append(len(word_tokenize(doc[i]))) #2d list\n",
        "\n"
      ],
      "metadata": {
        "id": "3W1sZ_MwSnsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}